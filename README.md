# TSMOM Carry Strategy Data Pipeline

![License](https://img.shields.io/badge/license-Proprietary-red.svg)
![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)
![Status](https://img.shields.io/badge/status-Production-green.svg)

A production-grade data pipeline for Time Series Momentum (TSMOM) and Carry strategies. This repository handles the robust downloading, processing, and normalization of futures data from Databento.

## ğŸš€ Features

-   **Robust Bulk Downloading**: Handles API rate limits, large file downloads, and job management via `src/download.py`.
-   **Parallel Processing**: High-performance data ingestion using `concurrent.futures` in `src/process_dataset.py`.
-   **Smart Symbology**: Automatically maps raw symbols to asset classes and regions using a centralized configuration.
-   **Production Logging**: Comprehensive logging to both console and `logs/` directory for full observability.
-   **Automation**: Standardized `Makefile` for one-command workflows.

## ğŸ“‚ Project Structure

```text
tsmom_carry/
â”œâ”€â”€ databento_data/       # Raw data storage (GitIgnored)
â”œâ”€â”€ logs/                 # Runtime logs
â”œâ”€â”€ src/                  # Source Code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py         # Universe & Contract Definitions
â”‚   â”œâ”€â”€ download.py       # Databento Download Logic
â”‚   â””â”€â”€ process_dataset.py # Data Processing & Merging
â”œâ”€â”€ tests/                # Test Suite
â”œâ”€â”€ _archive/             # Legacy scripts
â”œâ”€â”€ Makefile              # Automation
â”œâ”€â”€ pyproject.toml        # Project Configuration
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md             # Documentation
```

## ğŸ› ï¸ Installation

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/your-org/tsmom_carry.git
    cd tsmom_carry
    ```

2.  **Install dependencies**:
    ```bash
    make install
    ```

3.  **Set API Key**:
    ```bash
    export DATABENTO_API_KEY=your_api_key
    ```

## âš¡ Usage

### Download Data
Download the defined universe of futures contracts.
```bash
make download
```

### Process Data
Process raw files into a master dataset (`master_dataset.csv`).
```bash
make process
```

### Run Tests
Execute the test suite to verify integrity.
```bash
make test
```

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[Config: GLBX_UNIVERSE] --> B[Downloader]
    B -->|Submit Jobs| C[Databento API]
    C -->|Download .dbn.zst| D[databento_data/]
    D --> E[Processor]
    E -->|Parallel Load| F[Raw DataFrames]
    F -->|Merge & Enrich| G[Master Dataset]
    G --> H[master_dataset.csv]
```

## ğŸ“ Configuration

The asset universe is defined in `src/config.py`. To add new contracts, simply append a `RootContract` to the `GLBX_UNIVERSE` list:

```python
RootContract("GLBX.MDP3", "ES.FUT", "Equity", "US")
```

## ğŸ›¡ï¸ License

Proprietary and Confidential.